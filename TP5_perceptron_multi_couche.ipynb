{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/man237/man237/blob/main/TP5_perceptron_multi_couche.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6408f993",
      "metadata": {
        "id": "6408f993"
      },
      "source": [
        "# ÉTAPE 0 : preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3298d582",
      "metadata": {
        "id": "3298d582"
      },
      "source": [
        "a) Importez toutes les bibliothèques nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3ee559",
      "metadata": {
        "id": "4a3ee559",
        "outputId": "681ca6e8-eb8f-4df4-fa72-4c3869d144ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'imread' from 'scipy.misc' (C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-21-ec89af03dd9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abba65cb",
      "metadata": {
        "id": "abba65cb"
      },
      "source": [
        "b)\tDéfinissons une valeur de départ, afin que nous puissions contrôler le caractère aléatoire de nos modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04249744",
      "metadata": {
        "id": "04249744"
      },
      "outputs": [],
      "source": [
        "# To stop potential randomness\n",
        "seed = 128\n",
        "rng = np.random.RandomState(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c24f5c",
      "metadata": {
        "id": "91c24f5c"
      },
      "source": [
        "c)La première étape consiste à définir les chemins d'accès aux répertoires, par souci de sécurité !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876843c9",
      "metadata": {
        "id": "876843c9"
      },
      "outputs": [],
      "source": [
        "root_dir = os.path.abspath('../..') \n",
        "data_dir = os.path.join(root_dir, 'data') \n",
        "sub_dir = os.path.join(root_dir, 'sub') \n",
        "# check for existence os.path.exists(root_dir) os.path.exists(data_dir) os.path.exists(sub_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720371bc",
      "metadata": {
        "id": "720371bc"
      },
      "source": [
        "# ÉTAPE 1 : Chargement et prétraitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23a94bfc",
      "metadata": {
        "id": "23a94bfc"
      },
      "source": [
        "a)\tLisons maintenant nos ensembles de données. Ceux-ci sont au format .csv et ont un nom de fichier avec les étiquettes appropriées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06675fc9",
      "metadata": {
        "id": "06675fc9"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(data_dir, 'Test.csv'))\n",
        "\n",
        "sample_submission = pd.read_csv(os.path.join(data_dir, 'Sample_Submission.csv'))\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd8d6fe",
      "metadata": {
        "id": "5cd8d6fe"
      },
      "source": [
        "b)\tVoyons à quoi ressemblent nos données ! Nous lisons notre image et l'affichons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625e9ac0",
      "metadata": {
        "id": "625e9ac0"
      },
      "outputs": [],
      "source": [
        "img_name = rng.choice(train.filename)\n",
        "filepath = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
        "\n",
        "img = imread(filepath, flatten=True)\n",
        "\n",
        "pylab.imshow(img, cmap='gray')\n",
        "pylab.axis('off')\n",
        "pylab.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b94e44",
      "metadata": {
        "id": "20b94e44"
      },
      "source": [
        "c)\tL'image ci-dessus est représentée sous forme de tableau numpy , comme indiqué ci-dessous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7eaa42d",
      "metadata": {
        "id": "c7eaa42d"
      },
      "outputs": [],
      "source": [
        "imag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ccd315",
      "metadata": {
        "id": "b8ccd315"
      },
      "source": [
        "d)\tPour une manipulation plus facile des données, stockons toutes nos images sous forme de tableaux numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72157961",
      "metadata": {
        "id": "72157961"
      },
      "outputs": [],
      "source": [
        "temp = []\n",
        "for img_name in train.filename:\n",
        "    image_path = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
        "    img = imread(image_path, flatten=True)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "train_x = np.stack(temp)\n",
        "\n",
        "train_x /= 255.0\n",
        "train_x = train_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "temp = []\n",
        "for img_name in test.filename:\n",
        "    image_path = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
        "    img = imread(image_path, flatten=True)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "test_x = np.stack(temp)\n",
        "\n",
        "test_x /= 255.0\n",
        "test_x = test_x.reshape(-1, 784).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81fc0ef2",
      "metadata": {
        "id": "81fc0ef2"
      },
      "outputs": [],
      "source": [
        "train_y = keras.utils.np_utils.to_categorical(train.label.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6508c8",
      "metadata": {
        "id": "0a6508c8"
      },
      "source": [
        "e)\tComme il s'agit d'un problème typique de ML, pour tester le bon fonctionnement de notre modèle, nous créons un ensemble de validation. Prenons une taille fractionnée de 70:30 pour l'ensemble de train par rapport à l'ensemble de validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d637a3f",
      "metadata": {
        "id": "2d637a3f"
      },
      "outputs": [],
      "source": [
        "split_size = int(train_x.shape[0]*0.7)\n",
        "\n",
        "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
        "train_y, val_y = train_y[:split_size], train_y[split_size:]\n",
        "train.label.ix[split_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee71cab2",
      "metadata": {
        "id": "ee71cab2"
      },
      "source": [
        "# ÉTAPE 3: modéle de construction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97eafb90",
      "metadata": {
        "id": "97eafb90"
      },
      "source": [
        "a)\tVient maintenant l'essentiel ! Définissons notre architecture de réseau de neurones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851211c9",
      "metadata": {
        "id": "851211c9",
        "outputId": "6e76a82e-5bd8-41b5-af0d-6e2466883e6b"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "__init__() missing 1 required positional argument: 'units'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-32-f6ea8e7d1b93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m model = Sequential([\n\u001b[1;32m---> 16\u001b[1;33m   \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m   \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m ])\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
          ]
        }
      ],
      "source": [
        "# define vars\n",
        "input_num_units = 784\n",
        "hidden_num_units = 50\n",
        "output_num_units = 10\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "# import keras modules\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# create model\n",
        "model = Sequential([\n",
        "  Dense(output_dim=hidden_num_units, input_dim=input_num_units, activation='relu'),\n",
        "  Dense(output_dim=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
        "])\n",
        "\n",
        "# compile the model with necessary attributes\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4a63b0",
      "metadata": {
        "id": "3b4a63b0"
      },
      "source": [
        "b)\tIl est temps de former notre modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60735f24",
      "metadata": {
        "id": "60735f24"
      },
      "outputs": [],
      "source": [
        "trained_model = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=(val_x, val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff71c4b6",
      "metadata": {
        "id": "ff71c4b6"
      },
      "source": [
        "# ÉTAPE 4 : Évaluation du modèle\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb2df31",
      "metadata": {
        "id": "acb2df31"
      },
      "source": [
        "a)\tPour tester notre modèle de nos propres yeux, visualisons ses prédictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5425e86a",
      "metadata": {
        "id": "5425e86a"
      },
      "outputs": [],
      "source": [
        "pred = model.predict_classes(test_x)\n",
        "\n",
        "img_name = rng.choice(test.filename)\n",
        "filepath = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
        "\n",
        "img = imread(filepath, flatten=True)\n",
        "\n",
        "test_index = int(img_name.split('.')[0]) - train.shape[0]\n",
        "\n",
        "print \"Prediction is: \", pred[test_index]\n",
        "\n",
        "pylab.imshow(img, cmap='gray')\n",
        "pylab.axis('off')\n",
        "pylab.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70fca05c",
      "metadata": {
        "id": "70fca05c"
      },
      "source": [
        "b)\tNous voyons que notre modèle fonctionne bien même en étant très simple. Maintenant, nous créons une soumission avec notre modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31651ead",
      "metadata": {
        "id": "31651ead"
      },
      "outputs": [],
      "source": [
        "sample_submission.filename = test.filename; sample_submission.label = pred\n",
        "sample_submission.to_csv(os.path.join(sub_dir, 'sub02.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7801288",
      "metadata": {
        "id": "b7801288"
      },
      "source": [
        "# Hyperparamètres à surveiller dans les réseaux de neurones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aed09e",
      "metadata": {
        "id": "f4aed09e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b7446b",
      "metadata": {
        "id": "42b7446b"
      },
      "source": [
        "# Se salir les mains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3add88",
      "metadata": {
        "id": "af3add88"
      },
      "source": [
        "Comme nous l'avons fait avant, nous refaisons toutes les choses préalables. Importons les modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201f4b87",
      "metadata": {
        "id": "201f4b87"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa708eed",
      "metadata": {
        "id": "fa708eed"
      },
      "source": [
        "Comme précédemment, définissez la valeur de départ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d5e200",
      "metadata": {
        "id": "95d5e200"
      },
      "outputs": [],
      "source": [
        "# To stop potential randomness\n",
        "seed = 128\n",
        "rng = np.random.RandomState(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472a9cce",
      "metadata": {
        "id": "472a9cce"
      },
      "source": [
        "Définir des chemins pour une utilisation ultérieure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f19175",
      "metadata": {
        "id": "35f19175"
      },
      "outputs": [],
      "source": [
        "root_dir = os.path.abspath('../..')\n",
        "data_dir = os.path.join(root_dir, 'data')\n",
        "sub_dir = os.path.join(root_dir, 'sub')\n",
        "\n",
        "# check for existence\n",
        "os.path.exists(root_dir)\n",
        "os.path.exists(data_dir)\n",
        "os.path.exists(sub_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9567c815",
      "metadata": {
        "id": "9567c815"
      },
      "source": [
        "Définir des chemins pour une utilisation ultérieure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7db7366",
      "metadata": {
        "id": "e7db7366"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(data_dir, 'Test.csv'))\n",
        "\n",
        "sample_submission = pd.read_csv(os.path.join(data_dir, 'Sample_Submission.csv'))\n",
        "\n",
        "temp = []\n",
        "for img_name in train.filename:\n",
        "  image_path = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
        "  img = imread(image_path, flatten=True)\n",
        "  img = img.astype('float32')\n",
        "  temp.append(img)\n",
        "\n",
        "train_x = np.stack(temp)\n",
        "\n",
        "train_x /= 255.0\n",
        "train_x = train_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "temp = []\n",
        "for img_name in test.filename:\n",
        "  image_path = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
        "  img = imread(image_path, flatten=True)\n",
        "  img = img.astype('float32')\n",
        "  temp.append(img)\n",
        "\n",
        "test_x = np.stack(temp)\n",
        "\n",
        "test_x /= 255.0\n",
        "test_x = test_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "train_y = keras.utils.np_utils.to_categorical(train.label.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c994088",
      "metadata": {
        "id": "4c994088"
      },
      "source": [
        "Commençons notre peaufinage ! Changeons notre modèle pour qu'il soit \"large\", c'est-à-dire qu'il augmente le nombre de neurones dans notre couche cachée"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3725acf4",
      "metadata": {
        "id": "3725acf4"
      },
      "source": [
        "Divide our train data into training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87b376f",
      "metadata": {
        "id": "a87b376f"
      },
      "outputs": [],
      "source": [
        "split_size = int(train_x.shape[0]*0.7)\n",
        "\n",
        "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
        "train_y, val_y = train_y[:split_size], train_y[split_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c86926",
      "metadata": {
        "id": "54c86926"
      },
      "source": [
        "Commençons notre peaufinage ! Changeons notre modèle pour qu'il soit \"large\", c'est-à-dire qu'il augmente le nombre de neurones dans notre couche cachée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8131c7",
      "metadata": {
        "id": "ba8131c7"
      },
      "outputs": [],
      "source": [
        "# define vars\n",
        "input_num_units = 784\n",
        "hidden_num_units = 500\n",
        "output_num_units = 10\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "model = Sequential([\n",
        " Dense(output_dim=hidden_num_units, input_dim=input_num_units, activation='relu'),\n",
        "\n",
        " Dense(output_dim=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e84567",
      "metadata": {
        "id": "16e84567"
      },
      "source": [
        "Testons ce modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2c1aab",
      "metadata": {
        "id": "3e2c1aab"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_500 = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=(val_x, val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4f1bf2",
      "metadata": {
        "id": "8b4f1bf2"
      },
      "source": [
        "On voit que ce modèle fonctionne nettement mieux qu'avant ! Maintenant, au lieu de \"large\", nous essayons de rendre notre modèle \"profond\". Nous ajoutons quatre autres couches cachées avec 50 neurones chacune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8fdce1",
      "metadata": {
        "id": "0e8fdce1"
      },
      "outputs": [],
      "source": [
        "# define vars\n",
        "input_num_units = 784\n",
        "hidden1_num_units = 50\n",
        "hidden2_num_units = 50\n",
        "hidden3_num_units = 50\n",
        "hidden4_num_units = 50\n",
        "hidden5_num_units = 50\n",
        "output_num_units = 10\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "model = Sequential([\n",
        " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
        " Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='relu'),\n",
        " Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='relu'),\n",
        " Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='relu'),\n",
        " Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='relu'),\n",
        "\n",
        "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
        " ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8ee67f",
      "metadata": {
        "id": "9b8ee67f"
      },
      "source": [
        "Des suppositions sur la façon dont ce modèle fonctionnerait?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce68b4f6",
      "metadata": {
        "id": "ce68b4f6"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_5d = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=((val_x, val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a57949",
      "metadata": {
        "id": "83a57949"
      },
      "source": [
        "On dirait que nous n'avons pas obtenu ce que nous attendions. Cela peut être dû au fait que notre modèle est peut-être surajusté. Pour résoudre ce problème, nous utilisons une méthode appelée abandon. L'abandon consiste essentiellement à désactiver au hasard des parties du modèle afin qu'il ne \"surapprenne\" pas un concept (pour en savoir plus sur l'abandon, consultez l'article sur les concepts de base des réseaux de neurones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b20aa33",
      "metadata": {
        "id": "2b20aa33"
      },
      "outputs": [],
      "source": [
        "# define vars\n",
        "input_num_units = 784\n",
        "hidden1_num_units = 500\n",
        "hidden2_num_units = 500\n",
        "hidden3_num_units = 500\n",
        "hidden4_num_units = 500\n",
        "hidden5_num_units = 500\n",
        "output_num_units = 10\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "model = Sequential([\n",
        " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
        " Dropout(0.2),\n",
        " Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='relu'),\n",
        " Dropout(0.2),\n",
        " Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='relu'),\n",
        " Dropout(0.2),\n",
        " Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='relu'),\n",
        " Dropout(0.2),\n",
        " Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='relu'),\n",
        " Dropout(0.2),\n",
        "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
        " ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57641f9",
      "metadata": {
        "id": "b57641f9"
      },
      "source": [
        "Eh bien, je suis impatient de voir ce qui va se passer. Êtes-vous?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e0f084",
      "metadata": {
        "id": "a4e0f084"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "trained_model_deep_n_wide = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size,\n",
        "validation_data=(val_x, val_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0f226d",
      "metadata": {
        "id": "5d0f226d"
      },
      "outputs": [],
      "source": [
        "pred = model.predict_classes(test_x)\n",
        " sample_submission.filename = test.filename; sample_submission.label = pred\n",
        " sample_submission.to_csv(os.path.join(sub_dir, 'sub03.csv'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4deeea43",
      "metadata": {
        "id": "4deeea43"
      },
      "source": [
        "Comme dernier ajustement, nous allons essayer de changer le type de notre modèle. Jusqu'à présent, nous fabriquions des perceptrons multicouches (MLP). Changeons-le maintenant en un réseau de neurones convolutifs. (Pour obtenir une introduction approfondie au réseau de neurones convolutifs (CNN), parcourez cet article). Une chose nécessaire pour faire fonctionner un CNN est qu'il doit être organisé dans un format spécifique. Alors remodelons nos données et alimentons-les dans notre CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de7d980",
      "metadata": {
        "id": "9de7d980"
      },
      "outputs": [],
      "source": [
        "# reshape data\n",
        "train_x_temp = train_x.reshape(-1, 28, 28, 1)\n",
        "val_x_temp = val_x.reshape(-1, 28, 28, 1)\n",
        "# define vars\n",
        "input_shape = (784,)\n",
        "input_reshape = (28, 28, 1)\n",
        "conv_num_filters = 5\n",
        "conv_filter_size = 5\n",
        "pool_size = (2, 2)\n",
        "hidden_num_units = 50\n",
        "output_num_units = 10\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "model = Sequential([\n",
        " InputLayer(input_shape=input_reshape),\n",
        " Convolution2D(25, 5, 5, activation='relu'),\n",
        " MaxPooling2D(pool_size=pool_size),\n",
        " Convolution2D(25, 5, 5, activation='relu'),\n",
        " MaxPooling2D(pool_size=pool_size),\n",
        " Convolution2D(25, 4, 4, activation='relu'),\n",
        " Flatten(),\n",
        " Dense(output_dim=hidden_num_units, activation='relu'),\n",
        " Dense(output_dim=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "trained_model_conv = model.fit(train_x_temp, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=\n",
        "(val_x_temp, val_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f68c4f",
      "metadata": {
        "id": "b8f68c4f"
      },
      "outputs": [],
      "source": [
        "rrrrrrrrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78f82df",
      "metadata": {
        "id": "a78f82df"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f38a3c",
      "metadata": {
        "id": "88f38a3c"
      },
      "outputs": [],
      "source": [
        "# To stop potential randomness\n",
        "seed = 128\n",
        "rng = np.random.RandomState(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5699eea6",
      "metadata": {
        "id": "5699eea6"
      },
      "outputs": [],
      "source": [
        "root_dir = os.path.abspath('../..') \n",
        "data_dir = os.path.join(root_dir, 'data') \n",
        "sub_dir = os.path.join(root_dir, 'sub') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de3bb8b",
      "metadata": {
        "id": "8de3bb8b",
        "outputId": "259b6a8f-ab1b-4725-abd9-8f3858bff623"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO\\\\data\\\\Train\\\\train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-41-906a41c46b11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sample_Submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO\\\\data\\\\Train\\\\train.csv'"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(data_dir, 'Test.csv'))\n",
        "\n",
        "sample_submission = pd.read_csv(os.path.join(data_dir, 'Sample_Submission.csv'))\n",
        "\n",
        "train.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "TP_perceptron multi-couche.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}